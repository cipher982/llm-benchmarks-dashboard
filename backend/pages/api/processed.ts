import { NextApiRequest, NextApiResponse } from 'next';
import { CloudMetrics } from '../../models/BenchmarkMetrics';
import { processSpeedDistData, processTimeSeriesData, processRawTableData } from '../../utils/dataProcessing';
import { cleanTransformCloud } from '../../utils/processCloud';
import { corsMiddleware, fetchAndProcessMetrics } from '../../utils/apiMiddleware';
import logger from '../../utils/logger';
import { roundNumbers } from '../../utils/dataUtils';
import fs from 'fs/promises';
import path from 'path';

// Default time range in days
const DEFAULT_DAYS = 3;

// Request deduplication cache to prevent multiple identical expensive requests
const requestCache = new Map<string, Promise<any>>();

// Static file serving
async function tryServeStaticFile(days: number, res: NextApiResponse): Promise<boolean> {
    const filename = `processed-${days}days.json`;
    const filepath = path.join(process.cwd(), 'public', 'api', filename);
    
    // Add debug header with file path attempt
    res.setHeader('X-Static-Path-Attempted', filepath);
    
    try {
        // Check if file exists and get its stats
        const stats = await fs.stat(filepath);
        const ageMinutes = (Date.now() - stats.mtime.getTime()) / (1000 * 60);
        
        // Add debug headers
        res.setHeader('X-Static-File-Found', 'true');
        res.setHeader('X-Static-File-Age-Minutes', Math.floor(ageMinutes).toString());
        
        // Serve file if it's less than 2 hours old
        if (ageMinutes < 120) {
            const data = await fs.readFile(filepath, 'utf8');
            const parsedData = JSON.parse(data);
            
            // Add headers to indicate static file serving
            res.setHeader('Content-Type', 'application/json');
            res.setHeader('X-Cache-Status', 'STATIC-FILE');
            res.setHeader('X-Processing-Time', '1ms'); // Static files are instant
            
            logger.info(`üìÑ Served static file: ${filename} (${Math.floor(ageMinutes)}min old)`);
            res.status(200).json(parsedData);
            return true;
        } else {
            res.setHeader('X-Static-File-Status', 'STALE');
            logger.info(`‚è∞ Static file ${filename} is stale (${Math.floor(ageMinutes)}min old), using dynamic`);
        }
    } catch (error: any) {
        // File doesn't exist or other error - fall back to dynamic
        res.setHeader('X-Static-File-Found', 'false');
        res.setHeader('X-Static-File-Error', error.code || 'UNKNOWN');
        logger.info(`üìÑ Static file error for ${days} days: ${error.message}`);
    }
    
    return false;
}

function parseTimeRange(req: NextApiRequest) {
    const days = req.query.days ? parseInt(req.query.days as string) : DEFAULT_DAYS;
    return {
        days: Math.min(Math.max(days, 1), 90) // Limit between 1-90 days
    };
}

// This is the shared processing function used by both processed.ts and model.ts endpoints
// Performance optimization metrics are added here
export async function processAllMetrics(rawMetrics: any[], days: number) {
    const pipelineStartTime = process.hrtime.bigint();
    logger.info(`üöÄ processAllMetrics started with ${rawMetrics.length} raw metrics for ${days} days`);
    
    // Transform data first since other operations depend on it
    const startTime = process.hrtime.bigint();
    logger.info(`üîÑ Starting cleanTransformCloud on ${rawMetrics.length} metrics...`);
    const transformedData = cleanTransformCloud(rawMetrics);
    const endTime = process.hrtime.bigint();
    logger.info(`‚úÖ cleanTransformCloud took ${(endTime - startTime) / 1000000n}ms`);
    logger.info(`üìä Transformed data length: ${transformedData.length}`);

    // Apply model mapping to ALL data BEFORE processing (not just timeSeriesData)
    const mappingStartTime = process.hrtime.bigint();
    const useDbModels = process.env.USE_DATABASE_MODELS === 'true';
    logger.info(`üîß APPLYING MODEL MAPPING: useDbModels=${useDbModels}, transformedDataLength=${transformedData.length}`);
    
    const { mapModelNames } = await import('../../utils/modelMappingDB');
    logger.info(`üì¶ modelMappingDB module imported successfully`);
    let mappedData;
    try {
        mappedData = await mapModelNames(transformedData, useDbModels);
        const mappingEndTime = process.hrtime.bigint();
        logger.info(`üîß Model mapping took ${(mappingEndTime - mappingStartTime) / 1000000n}ms, mappedDataLength=${mappedData.length}`);
    } catch (mappingError) {
        logger.error(`‚ùå Model mapping failed: ${mappingError}`);
        logger.error(`üìç Mapping error stack: ${mappingError instanceof Error ? mappingError.stack : 'No stack available'}`);
        throw mappingError;
    }

    // Run the processing operations in parallel
    // IMPORTANT: These three data structures serve different UI purposes:
    // - speedDistData: ALL provider-model combinations - each gets own distribution curve
    // - timeSeriesData: UNIQUE models only - each gets chart with multiple provider lines  
    // - tableData: ALL provider-model combinations - raw data table with clean names
    const startTimeParallel = process.hrtime.bigint();
    logger.info(`üöÄ Starting parallel processing with ${mappedData.length} mapped metrics...`);
    
    let speedDistData, timeSeriesData, tableData;
    try {
        [speedDistData, timeSeriesData, tableData] = await Promise.all([
            (async () => {
                const start = process.hrtime.bigint();
                logger.info(`üîÑ Starting processSpeedDistData...`);
                const result = await processSpeedDistData(mappedData);
                const end = process.hrtime.bigint();
                logger.info(`‚úÖ processSpeedDistData took ${(end - start) / 1000000n}ms, result length: ${result.length}`);
                return result;
            })(),
            (async () => {
                const start = process.hrtime.bigint();
                logger.info(`üîÑ Starting processTimeSeriesData...`);
                const result = await processTimeSeriesData(mappedData, days);
                const end = process.hrtime.bigint();
                logger.info(`‚úÖ processTimeSeriesData took ${(end - start) / 1000000n}ms, models: ${result.models?.length || 0}`);
                return result;
            })(),
            (async () => {
                const start = process.hrtime.bigint();
                logger.info(`üîÑ Starting processRawTableData...`);
                const result = await processRawTableData(mappedData);
                const end = process.hrtime.bigint();
                logger.info(`‚úÖ processRawTableData took ${(end - start) / 1000000n}ms, rows: ${result.length}`);
                return result;
            })()
        ]);
        const endTimeParallel = process.hrtime.bigint();
        logger.info(`üéØ All parallel processing took ${(endTimeParallel - startTimeParallel) / 1000000n}ms`);
    } catch (parallelError) {
        logger.error(`‚ùå Parallel processing failed: ${parallelError}`);
        logger.error(`üìç Parallel error stack: ${parallelError instanceof Error ? parallelError.stack : 'No stack available'}`);
        throw parallelError;
    }

    // Log sizes for analysis
    if (speedDistData.length > 0) {
        logger.info(`First model density points sample: ${
            JSON.stringify(speedDistData[0].density_points.slice(0, 3))
        }`);
    }
    logger.info('üìè Data sizes (KB):');
    logger.info(`Speed Distribution: ${(JSON.stringify(speedDistData).length / 1024).toFixed(2)}`);
    logger.info(`Time Series: ${(JSON.stringify(timeSeriesData).length / 1024).toFixed(2)}`);
    logger.info(`Table: ${(JSON.stringify(tableData).length / 1024).toFixed(2)}`);

    const finalResult = roundNumbers({
        speedDistribution: speedDistData,
        timeSeries: timeSeriesData,
        table: tableData
    });
    
    const pipelineEndTime = process.hrtime.bigint();
    logger.info(`üèÅ Total processAllMetrics pipeline took ${(pipelineEndTime - pipelineStartTime) / 1000000n}ms`);
    
    return finalResult;
}

async function handler(
    req: NextApiRequest,
    res: NextApiResponse
) {
    if (req.method !== "GET") {
        return res.status(405).json({ error: "Method not allowed" });
    }

    const requestStartTime = Date.now();
    
    try {
        // Parse time range
        const timeRange = parseTimeRange(req);
        const days = timeRange.days;
        
        // Check if static file serving should be bypassed
        const useStatic = req.query.bypass_static !== "true"; // Static enabled by default
        
        // First priority: Try to serve static file (unless bypassed)
        if (useStatic) {
            const staticServed = await tryServeStaticFile(days, res);
            if (staticServed) {
                return; // Response already sent
            }
        }
        
        // If we reach here, static file was not available - generate dynamically
        logger.info(`Static file not available for days=${days}, generating dynamically`);
        
        // Implement request deduplication to prevent race conditions
        const cacheKey = `processed-${days}days`;
        
        if (requestCache.has(cacheKey)) {
            logger.info(`üîÑ Request deduplication: using cached promise for ${cacheKey}`);
            const cachedPromise = requestCache.get(cacheKey)!;
            const processedData = await cachedPromise;
            
            // Add debugging headers
            res.setHeader("X-Cache-Status", "DEDUPLICATION-CACHE");
            res.setHeader("X-Model-Mapping", process.env.USE_DATABASE_MODELS === 'true' ? "database" : "hardcoded");
            res.setHeader("X-Processing-Time", `${Date.now() - requestStartTime}ms`);
            
            return res.status(200).json(processedData);
        }
        
        // Create new request promise with timeout and cache it
        const requestPromise = (async () => {
            try {
                // Add timeout for large datasets (5 minutes max)
                const timeoutPromise = new Promise((_, reject) => {
                    setTimeout(() => reject(new Error('Request timeout after 5 minutes')), 5 * 60 * 1000);
                });
                
                const dataPromise = (async () => {
                    // Fetch raw data from MongoDB
                    const rawData = await fetchAndProcessMetrics(
                        CloudMetrics,
                        days,
                        (data: any[]) => data
                    );
                    
                    // Process the data
                    const metricsArray = Array.isArray(rawData) ? rawData : (rawData.raw || []);
                    
                    if (!metricsArray.length) {
                        logger.warn(`No metrics found for days=${days}`);
                        throw new Error('No metrics found');
                    }
                    
                    logger.info(`Processing ${metricsArray.length} metrics`);
                    return await processAllMetrics(metricsArray, days);
                })();
                
                // Race between data processing and timeout
                return await Promise.race([dataPromise, timeoutPromise]);
            } finally {
                // Clean up cache after processing (success or failure)
                requestCache.delete(cacheKey);
            }
        })();
        
        // Cache the promise
        requestCache.set(cacheKey, requestPromise);
        logger.info(`üîÑ Request deduplication: cached new promise for ${cacheKey}`);
        
        // Wait for the result
        const processedData = await requestPromise;
        
        // Add debugging headers
        res.setHeader("X-Cache-Status", "DYNAMIC-GENERATION");
        res.setHeader("X-Model-Mapping", process.env.USE_DATABASE_MODELS === 'true' ? "database" : "hardcoded");
        res.setHeader("X-Processing-Time", `${Date.now() - requestStartTime}ms`);
        
        // Return the processed data
        return res.status(200).json(processedData);
    } catch (error) {
        logger.error(`Error processing metrics: ${error}`);
        
        // Clean up cache on error if needed
        const timeRange = parseTimeRange(req);
        const cacheKey = `processed-${timeRange.days}days`;
        if (requestCache.has(cacheKey)) {
            requestCache.delete(cacheKey);
        }
        
        return res.status(500).json({ error: 'Failed to process metrics' });
    }
}

// Wrap the handler with the existing CORS middleware
export default async function processedHandler(req: NextApiRequest, res: NextApiResponse) {
    // Handle CORS preflight
    const corsHandled = await corsMiddleware(req, res);
    if (corsHandled) return;

    // Handle the actual request
    return handler(req, res);
}